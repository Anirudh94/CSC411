from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.utils.np_utils import to_categorical
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from keras.callbacks import TensorBoard

from util import load_data, load_X, writeCSV

import numpy as np

# dimensions of our images.
img_width, img_height, img_channels = 128, 128, 3
nb_classes = 8
nb_epoch = 15
batch_size = 32

# load training data
X, y = load_data('./train', 'train.csv')
X_val = load_X('./val')
print('X_val.shape')
print(X_val.shape)

X_train = X[:6000]
y_train = y[:6000]
X_test = X[6000:]
y_test = y[6000:]

# convert data to one-hot
y_train = to_categorical(y_train - 1, nb_classes=nb_classes)
y_test = to_categorical(y_test - 1, nb_classes=nb_classes)

# build ConvNet
model = Sequential()

model.add(AveragePooling2D(pool_size=(2, 2), input_shape=(img_width, img_height, X_train.shape[3])))
model.add(Convolution2D(32, 3, 3))

#model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height, X_train.shape[3])))
model.add(Activation('relu'))
model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

# let's train the model using SGD + momentum (how original).
lrate = 0.005
sgd = SGD(lr=lrate, decay=1e-4, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
				optimizer=sgd,
				metrics=['accuracy'])

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_val = X_val.astype('float32')
X_train /= 255
X_test /= 255
X_val /= 255

'''
model.fit(X_train, y_train,
            nb_epoch=nb_epoch,
            batch_size=batch_size,
            validation_data=(X_test, y_test))
'''

model.load_weights('weights.h5')

datagen = ImageDataGenerator(
        rotation_range=20,
        shear_range=0.2,
        zoom_range=0.2,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True)

# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(X_train)


tb = TensorBoard(log_dir='./log')

# fit the model on the batches generated by datagen.flow()
model.fit_generator(datagen.flow(X_train, y_train,
		batch_size=batch_size),
		samples_per_epoch=X_train.shape[0],
		nb_epoch=nb_epoch,
		validation_data=(X_test, y_test),
                callbacks=[tb])

'''

pred = model.predict(X_test, batch_size=32, verbose=1)
pred_s = np.argmax(pred, axis=1) + 1
y_test_s = np.argmax(y_test, axis=1) + 1


for i in range(1,9):	
	print(np.sum(pred_s == i))

for i in range(1,9):	
	print(np.sum(y_test_s == i))

for i in range(1,9):	
	print((np.where(np.logical_and((y_test_s - pred_s)!= 0 ,y_test_s ==i))[0]+6000).shape)
	print((np.where(np.logical_and((y_test_s - pred_s)!= 0 ,pred_s ==i))[0]+6000).shape)
	print(np.where(np.logical_and((y_test_s - pred_s)!= 0 ,y_test_s ==i))[0]+6000)
#writeCSV('submission.csv', pred)
'''

model.save_weights('weights.h5')

from keras.models import Sequential
from keras.layers import Convolution2D, MaxPooling2D, AveragePooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.utils.np_utils import to_categorical
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img

from util import load_data, load_X, writeCSV

import numpy as np

# dimensions of our images.
img_width, img_height, img_channels = 128, 128, 3
nb_classes = 8
nb_epoch = 100
batch_size = 32

# load training data
X, y = load_data('./train', 'train.csv')
X_val = load_X('./val')
print('X_val.shape')
print(X_val.shape)

X_train = X[:6000]
y_train = y[:6000]
X_test = X[6000:]
y_test = y[6000:]

# convert data to one-hot
y_train = to_categorical(y_train - 1, nb_classes=nb_classes)
y_test = to_categorical(y_test - 1, nb_classes=nb_classes)

# build ConvNet
model = Sequential()

model.add(AveragePooling2D(pool_size=(2, 2), input_shape=(img_width, img_height, img_channels)))
model.add(Convolution2D(32, 3, 3))

#model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height, X_train.shape[3])))
model.add(Activation('relu'))
model.add(Convolution2D(32, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(Convolution2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(nb_classes))
model.add(Activation('softmax'))

# let's train the model using SGD + momentum (how original).
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=False)
model.compile(loss='categorical_crossentropy',
				optimizer=sgd,
				metrics=['accuracy'])

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_val = X_val.astype('float32')
X_train /= 255
X_test /= 255
X_val /= 255

'''
model.fit(X_train, y_train,
            nb_epoch=nb_epoch,
            batch_size=batch_size,
            validation_data=(X_test, y_test))
datagen = ImageDataGenerator(
        shear_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True)


# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(X_train)

# fit the model on the batches generated by datagen.flow()
model.fit_generator(datagen.flow(X_train, y_train,
		batch_size=batch_size),
		samples_per_epoch=X_train.shape[0],
		nb_epoch=nb_epoch,
		validation_data=(X_test, y_test))
'''
model.load_weights('weights.h5')

pred = model.predict(X_val, batch_size=32, verbose=1)
pred = np.argmax(pred, axis=1) + 1

for i in range(1,9):	
	print(np.sum(pred == i))

writeCSV('submission.csv', pred)

#model.save_weights('weights.h5')
